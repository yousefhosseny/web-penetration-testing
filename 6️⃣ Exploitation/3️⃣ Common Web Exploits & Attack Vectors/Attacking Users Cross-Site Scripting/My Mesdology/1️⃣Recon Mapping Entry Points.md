Google Dorks
## ✅ المرحلة 1: **اكتشاف وإيجاد نقاط الإدخال (Input Mapping)**

### 📌 الفكرة:

لازم نعرف كل الأماكن اللي بنقدر نحط فيها مدخلات (inputs) في الموقع. يعني كل فورم، كل باراميتر في الرابط، الهيدر، الكوكيز… كل حاجة المستخدم بيبعتها للسيرفر.

#### 🔎 **Goal**: Find **every single injection point**.

| Input Vectors    | Example                           |
| ---------------- | --------------------------------- |
| URL query params | `?search=INJECTME`                |
| POST body fields | Form inputs                       |
| JSON keys/values | `{"user":"INJECTME"}`             |
| HTTP headers     | `Referer`, `User-Agent`, `Origin` |
| Cookies          | `Set-Cookie`, `authToken`         |
| DOM elements     | `window.location.hash`            |

### 🛠 الأدوات:

- `ParamSpider`, `Arjun`: بيجيبوا الباراميترز اللي الموقع بيستقبلها.
    
- `waybackurls`, `gau`: بيجيبوا روابط قديمة من Wayback Machine.
    
- تحليل ملفات JavaScript باستخدام `LinkFinder`.
    

# الخطوات 
## 1. اجمع ال  SubDomains الهدف (مثلاً `example.com`)
##### -  (Active discovery)    استخدم الادوات التاليه  الطريقه دي اسمها   
- 
	- [[DNSRecon]] 
	- [[Amass]]
	- [[ffuf]]
	- [[Sublist3r]]

##### - ممكن اجمع ال SubDomains بطرق تانيه منغير استخدام ادوات  اسمها  (Passive discovery )

- https://crt.sh/
- [[WHOIS]] او  https://who.is/
- [[Google Dorks]]
-    [Wayback Machine](https://web.archive.org/).
-  [crt.sh](https://crt.sh/) 

###### هام :
طبعا كل النتائج اللي هتطلع هخزنها في ملف اسمه  ` SubDomains.txt` علشان هستخدمهم في الخطوات التاليه . 
###### هام 
الخطوات التاليه هعملها علي كل SubDomains لوحده يعني انا هاخد كل SubDomains هعمل فيه كل  الخطوات التاليه لوحده ثم اخش علي ال SubDomains اللي بعده  .

----
## 2. ء find URL query params
##### 1. اول حاجه هتعمل ملف اسمه `All_urls.txt`  هحط فيه كل ال URL وال parameter's اللي  في الموقع من الخطوات التاليه
- 
	### 1. هستخدم ال Burp Suite ( Crawler /param miner)  ==> ء
	1. ثم هستخدم ال Katana 
	2. ثم هستخدم ال ParamSpider
	3. ثم هستخدم ال Gau
	4. ثم هستخدم ال  arjun
	5.  ثم هستخدم ال  waybackurls
	6.  ثم هستخدم  كل الطرق اليدويه  
	
#####  2. تصفية وإزالة التكرار  هنا هظبت الملف `All_urls.txt` هعدل عليه  بالخطوات التاليه 

- 
	###### 1. إزالة التكرار
	```bash
     cat All_urls.txt | sort -u > unique_urls.txt
     ```
	###### 2. تصفية الروابط اللي فيها Parameters: 
	 ```bash
     cat unique_urls.txt | grep "?" > final_params_urls.txt
     ```
	###### 3. لو عايز المعلمات فقط، استخدمي GF:
	 ```bash
     cat final_params_urls.txt | gf parameters > final_params.txt
     ```
	###### 4. استخدم  httpx لفحص الروابط والتأكد إنها تعمل:
  ```bash
       httpx -l unique_urls.txt -sc -title -o httpx_results.txt
          cat httpx_results.txt | grep "200" > final_params_urls.txt
       ```


----
## 3.  Form fields (POST)

هو عباره عن الحقول اللي بحط فيها البيانات في الموقع طب انا بدور علي اي  ==> انا بدور علي كل الصفحات اللي بتعمل `POST` 

#### 1. اول حاجه هتعمل ملف اسمه `All_urls.txt`  هحط فيه كل ال URL وال parameter's اللي  في الموقع من الخطوات التاليه
- 
	 1. هستخدم ال Burp Suite ( Crawler)  ==> ء
	2. ثم هستخدم ال Katana 
	3. ثم هستخدم ال  arjun
	4.  ثم هستخدم  كل الطرق اليدويه 
	5.   ثم تحليل Swagger/OpenAPI (لطلبات API)

#### 2. تصفية وإزالة التكرار  هنا هظبت الملف `All_urls.txt` هعدل عليه  بالخطوات التاليه 

- 
	 1. إزالة التكرار:
		```bash
     cat All_urls.txt | sort -u > unique_urls.txt
     ```
	 2. استخدمي httpx لتحديد الروابط اللي بتدعم POST:
	 ```bash
    httpx -l katana_urls.txt -mr "POST" -sc -o httpx_post_urls.txt
     ```
	 3. **استخراج الحقول** اللي هحط فيها البايلود استخدم السكريبت التالي 
```python
       import requests
       with open('httpx_post_urls.txt', 'r') as file:
           for line in file:
               url = line.strip().split('[')[0].strip()
               try:
                   response = requests.post(url, data={'test':'value'})
                   print(f"URL: {url}\nResponse: {response.text[:200]}")
               except:
                   pass
       ```
	

----
## 4.JSON keys
هنا انا بدور علي حاجه اسمها JSON keys شكلها بيبقي عامل كده `  {"user_id":123, "token":"abc"}`
#### 1. اول حاجه هتعمل ملف اسمه`All_Key.txt`  هحط فيه كل ال UR اللي  JSON keys في الموقع من الخطوات التاليه
- 
	 1. هتعمل  Network Requests عن طريق DevTools
	2. استخدام أدوات اعتراض الطلبات مثل  **Burp Suite**
	3.  تحليل وثائق API (API Documentation) عن طريق  **Swagger UI** و ال **Postman**
	4.  فحص ملفات JavaScript**  عن طريق   **LinkFinder**  و ال**JSFinder** 
	5. ثم هستخدم ال  arjun 
	6.  ثم هستخدم  اختبار النقاط النهائية يدويًا (Manual Endpoint Testing)
	7.   ثم  فحص استجابات الخادم (Server Responses)
	8. ثم هستخدم  التخمين الذكي (Smart Guessing)
	9. ثم تحليل التطبيقات أحادية الصفحة (SPA Analysis)
	10. فحص ملفات التكوين أو التسريبات (Configuration Files/Leaks)
